{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import glob\n",
    "import math\n",
    "import random\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import shutil\n",
    "import keras\n",
    "\n",
    "from config import config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tuning\n",
    "dataset = config.DATASET\n",
    "n_points = config.N_POINTS\n",
    "cell = config.CELL\n",
    "num_classes = config.NUM_CLASSES\n",
    "extension = config.EXTENSION\n",
    "class_name = config.CLASS_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_or_test = [\"train\",\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#読み込み，書き込み用ディレクトリ\n",
    "data_dir = config.DATA_DIR\n",
    "vox_dir = config.VOX_DIR\n",
    "fig_dir =  config.FIG_DIR\n",
    "dist_dir = config.DIST_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_list = [data_dir , vox_dir , fig_dir , dist_dir]\n",
    "for directory in dir_list:\n",
    "    if os.path.exists(directory) == False:\n",
    "        os.makedirs(directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TrainとTestのデータdirectory作成\n",
    "for t in train_or_test:\n",
    "    for cl in class_name:\n",
    "        if os.path.exists(data_dir + cl + \"/\" + t + \"/\") == False:\n",
    "            os.makedirs(data_dir + cl + \"/\" + t + \"/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def load_off(filename):\n",
    "    # read OFF file\n",
    "    with open(filename,\"r\") as handle:\n",
    "        off = handle.read().rstrip().split(\"\\n\")\n",
    "        \n",
    "    #OFFファイルが不正かどうか判定\n",
    "    if off[0] != \"OFF\":\n",
    "#         print(\"{} is broken!!\".format(filename))\n",
    "        params = list(off[0].split(\" \"))\n",
    "        n_vertices = int(params[0].strip(\"OFF\"))\n",
    "        n_faces = int(params[1])\n",
    "        off.insert(0, \"OFF\")\n",
    "    \n",
    "    else:\n",
    "        #get params and faces\n",
    "        params = list(map(int, off[1].split(\" \")))\n",
    "        n_vertices = params[0]\n",
    "        n_faces = params[1]\n",
    "\n",
    "    # read  Vertex coordinates\n",
    "    vertices = []\n",
    "    for n in range(n_vertices):\n",
    "        coords = list(map(float, off[2+n].split()))\n",
    "        vertices.append(coords)\n",
    "\n",
    "    # read information of faces\n",
    "    faces = []\n",
    "    for n in range(n_faces):\n",
    "        connects = list(map(int, off[2 + n_vertices + n].split(\" \")[1:4]))\n",
    "        faces.append(connects)\n",
    "\n",
    "    return vertices, faces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def load_stl(filename):\n",
    "    # read STL file\n",
    "    with open(filename,\"r\") as handle:\n",
    "        stl = handle.read().rstrip().split(\"\\n\")\n",
    "        \n",
    "    #get vertice\n",
    "    vertice = []\n",
    "    for i in range(len(stl)):\n",
    "        pool = stl[i].split()\n",
    "        if pool[0] == \"vertex\":\n",
    "            vertex = list(map(float, pool[1:]))\n",
    "            vertice.append(vertex)\n",
    "            \n",
    "    seen = []\n",
    "    unique_vertice = [x for x in vertice if x not in seen and not seen.append(x)]\n",
    "            \n",
    "    #get faces\n",
    "    faces = []\n",
    "    for n in range(len(stl)):\n",
    "        if stl[n].split() == ['outer', 'loop']:\n",
    "            indexes = []\n",
    "            for i in range(3):\n",
    "                index = get_index_2d_list(unique_vertice, list(map(float,stl[n+i+1].split()[1:])))\n",
    "                indexes.append(index)\n",
    "            faces.append(indexes)\n",
    "\n",
    "    return unique_vertice, faces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def generate_points(points, face_ids, num_generated_point=1000):\n",
    "    \n",
    "    points = np.array(points)\n",
    "    face_ids = np.array(face_ids)\n",
    "    \n",
    "    # compute area\n",
    "    vec1 = points[face_ids[:,0]] - points[face_ids[:,1]]\n",
    "    vec2 = points[face_ids[:,1]] - points[face_ids[:,2]]\n",
    "    cross_product = np.cross(vec1, vec2)\n",
    "    area = np.sqrt(np.power(cross_product, 2).sum(axis=1))\n",
    "\n",
    "    # cumsum area\n",
    "    cum_area = np.cumsum(area)\n",
    "\n",
    "    # generate random \n",
    "    random = np.random.rand(num_generated_point) * cum_area[-1]\n",
    "\n",
    "    # convert random to index\n",
    "    #random_idx = [bisect.bisect_left(cum_area, x) for x in random]\n",
    "    random_idx = np.random.choice(np.arange(len(face_ids)), size=num_generated_point, p=area/cum_area[-1])\n",
    "\n",
    "    #import pdb; pdb.set_trace()\n",
    "\n",
    "    # generate points\n",
    "    r1 = np.tile(np.random.rand(num_generated_point), (3, 1)).T\n",
    "    r2 = np.tile(np.random.rand(num_generated_point), (3, 1)).T\n",
    "\n",
    "    generated_points = (1 - np.sqrt(r1))*points[face_ids[random_idx, 0]] + np.sqrt(r1)*(1-r2)*points[face_ids[random_idx, 1]] + np.sqrt(r1)*r2*points[face_ids[random_idx, 2]]\n",
    "\n",
    "    return generated_points, random_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def voxilize(np_pc,cell):\n",
    "# ボクセル化した配列を返す\n",
    "    max_dist = 0.0\n",
    "    for it in range(0,3):\n",
    "        # 最大値と最小値の距離を求める\n",
    "        min_ = np.amin(np_pc[:,it])\n",
    "        max_ = np.amax(np_pc[:,it])\n",
    "        dist = max_-min_\n",
    "\n",
    "        #xyzで一番並行距離が大きいのを求める\n",
    "        if dist > max_dist:\n",
    "            max_dist = dist\n",
    "            \n",
    "    for it in range(0,3):\n",
    "\n",
    "        # 最大値と最小値の距離を求める\n",
    "        min_ = np.amin(np_pc[:,it])\n",
    "        max_ = np.amax(np_pc[:,it])\n",
    "        dist = max_-min_\n",
    "        \n",
    "        #中心座標を 0,0,0にセットする（原点が中心にくるようにする）\n",
    "        np_pc[:,it] = np_pc[:,it] - dist/2 - min_\n",
    "\n",
    "        #covered cell\n",
    "        cls = cell - 3\n",
    "\n",
    "        #ボクセル一個当たりのサイズを求める\n",
    "        vox_sz = max_dist/(cls-1)\n",
    "\n",
    "        #上で算出した値で各点を割る。これで各点は(-14, 14)の範囲の値になる\n",
    "        np_pc[:,it] = np_pc[:,it]/vox_sz\n",
    "\n",
    "        #各点が全て正の整数になるよう移動。これで各点は[0, 30]になる（多分）\n",
    "        np_pc[:,it] = np_pc[:,it] + (cls-1)/2\n",
    "\n",
    "\n",
    "    #整数にする\n",
    "    np_pc = np.rint(np_pc).astype(np.uint32)\n",
    "\n",
    "\n",
    "    #３０＊３０＊３０の配列を作り，点が存在する場合は1、存在しない場合は0を入力する。\n",
    "    vox = np.zeros([cell-2,cell-2,cell-2])\n",
    "\n",
    "    # (pc_x, pc_y, pc_z)にnp_pcの座標を代入する\n",
    "    for (pc_x, pc_y, pc_z) in np_pc:\n",
    "\n",
    "#     # 点が存在しても20%の確率で0とし、データにノイズを加え、汎用性を上げている\n",
    "#     # ここ，ノイジーなデータの場合８０よりも小さい数字にしたほうがいいかもね？\n",
    "#         if random.randint(0,100) < 80:\n",
    "        vox[pc_x, pc_y, pc_z] = 1\n",
    "\n",
    "    np_vox = np.zeros([1,cell,cell,cell,1])\n",
    "    np_vox[0, 1:-1, 1:-1, 1:-1,0] = vox\n",
    "\n",
    "    return np_vox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def voxel_scatter(np_vox):\n",
    "# キレイに整形するやつ\n",
    "    #空の配列を作る\n",
    "    vox_scat = np.zeros([0,3], dtype= np.uint32)\n",
    "\n",
    "    #32回\n",
    "    for x in range(0,np_vox.shape[1]):\n",
    "        #32回\n",
    "        for y in range(0,np_vox.shape[2]):\n",
    "            #32回\n",
    "            for z in range(0,np_vox.shape[3]):\n",
    "                #（ｘ，ｙ，ｚ）に１が入っていればその座標を返す\n",
    "                if np_vox[0,x,y,z,0] == 1.0:\n",
    "                    arr_tmp = np.zeros([1,3],dtype=np.uint32)\n",
    "                    arr_tmp[0,:] = (x,y,z)\n",
    "                    vox_scat = np.concatenate((vox_scat,arr_tmp))\n",
    "    return vox_scat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def load_vox_stl(filename, n_points = n_points):\n",
    "    #shape(N,1,32,32,32)の配列を返す\n",
    "    vertices, faces = load_stl(filename)\n",
    "    points,faces = generate_points(vertices, faces, n_points)\n",
    "    # ValueError: sequence too large; cannot be greater than 32の回避策\n",
    "    # list 2 numpy.ndarray\n",
    "    pc = np.empty((len(points), len(points[0])))\n",
    "    pc[:] = points\n",
    "    vox = voxilize(pc,cell)\n",
    "\n",
    "    return vox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def load_vox_off(filename, n_points = n_points):\n",
    "    #shape(N,1,32,32,32)の配列を返す\n",
    "    vertices, faces = load_off(filename)\n",
    "    points,faces = generate_points(vertices, faces, n_points)\n",
    "    # ValueError: sequence too large; cannot be greater than 32の回避策\n",
    "    # list 2 numpy.ndarray\n",
    "    pc = np.empty((len(points), len(points[0])))\n",
    "    pc[:] = points\n",
    "    vox = voxilize(pc,cell)\n",
    "\n",
    "    return vox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# XYZ 長さの取得\n",
    "def calc_dist(filename):\n",
    "    vertices, faces = load_stl(filename)\n",
    "    point =  np.array(vertices)\n",
    "    dist = np.zeros(3)\n",
    "    for i in range(0,3):\n",
    "        min_ = np.amin(point[:,i])\n",
    "        max_ = np.amax(point[:,i])\n",
    "        dist_ = max_ - min_\n",
    "        if dist_ < 0.001:\n",
    "            dist_ = 0.001\n",
    "        dist[i] = dist_\n",
    "    return dist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ディレクトリ操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Renameing bathtub files ...\n",
      "train is already renamed!!\n",
      "test is already renamed!!\n",
      "Renameing chair files ...\n",
      "train is already renamed!!\n",
      "test is already renamed!!\n",
      "Renameing dresser files ...\n",
      "train is already renamed!!\n",
      "test is already renamed!!\n",
      "Renameing night_stand files ...\n",
      "train is already renamed!!\n",
      "test is already renamed!!\n",
      "Renameing table files ...\n",
      "train is already renamed!!\n",
      "test is already renamed!!\n",
      "Renameing bed files ...\n",
      "train is already renamed!!\n",
      "test is already renamed!!\n",
      "Renameing desk files ...\n",
      "train is already renamed!!\n",
      "test is already renamed!!\n",
      "Renameing monitor files ...\n",
      "train is already renamed!!\n",
      "test is already renamed!!\n",
      "Renameing sofa files ...\n",
      "train is already renamed!!\n",
      "test is already renamed!!\n",
      "Renameing toilet files ...\n",
      "train is already renamed!!\n",
      "test is already renamed!!\n",
      "Rename all!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if dataset == \"3D_SAMPLE\":\n",
    "    # train_or_testの分ける\n",
    "    for cl in class_name:\n",
    "        namelist = glob.glob(data_dir+ cl +\"/*\")\n",
    "        namelist.remove(data_dir+ cl + \"/train\")\n",
    "        namelist.remove(data_dir+ cl + \"/test\")\n",
    "        for i in range(len(namelist)):\n",
    "            if i%5 == 0 :\n",
    "                shutil.move(namelist[i], data_dir + cl + \"/test/.\")\n",
    "    #             print(namelist[i])\n",
    "            else:\n",
    "                shutil.move(namelist[i], data_dir + cl + \"/train/.\")\n",
    "    #             print(namelist[i])\n",
    "#  ====================rename===============================\n",
    "for cl in class_name:\n",
    "    print('Renameing {} files ...'.format(cl))\n",
    "    for t in train_or_test:\n",
    "        i = 0\n",
    "        #ファイル名を取得\n",
    "        namelist = glob.glob(data_dir+ cl +\"/\" + t + \"/*\")\n",
    "        if data_dir+ cl +\"/\" + t + \"/0\" + extension in namelist:\n",
    "            print(t +\" is already renamed!!\")\n",
    "            continue\n",
    "        for file in namelist:\n",
    "            os.rename(file, data_dir+ cl +'/' + t +\"/\"+ str(i) + extension)\n",
    "            i+=1\n",
    "print(\"Rename all!\")\n",
    "dir_rename_flag = True\n",
    "#  ====================rename==============================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ボクセルのnumpy保存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7288ee3cb56546d3adbdb928cc596676",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=10), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bathtub\n",
      "ModelNet10/bathtubtrain.npy is exist.\n",
      "chair\n",
      "ModelNet10/chairtrain.npy is exist.\n",
      "dresser\n",
      "ModelNet10/dressertrain.npy is exist.\n",
      "night_stand\n",
      "ModelNet10/night_standtrain.npy is exist.\n",
      "table\n",
      "ModelNet10/tabletrain.npy is exist.\n",
      "bed\n",
      "ModelNet10/bedtrain.npy is exist.\n",
      "desk\n",
      "ModelNet10/desktrain.npy is exist.\n",
      "monitor\n",
      "ModelNet10/monitortrain.npy is exist.\n",
      "sofa\n",
      "ModelNet10/sofatrain.npy is exist.\n",
      "toilet\n",
      "ModelNet10/toilettrain.npy is exist.\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e8e4afcd7544ade9437c0ce048b888a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=10), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bathtub\n",
      "ModelNet10/bathtubtest.npy is exist.\n",
      "chair\n",
      "ModelNet10/chairtest.npy is exist.\n",
      "dresser\n",
      "ModelNet10/dressertest.npy is exist.\n",
      "night_stand\n",
      "ModelNet10/night_standtest.npy is exist.\n",
      "table\n",
      "ModelNet10/tabletest.npy is exist.\n",
      "bed\n",
      "ModelNet10/bedtest.npy is exist.\n",
      "desk\n",
      "ModelNet10/desktest.npy is exist.\n",
      "monitor\n",
      "ModelNet10/monitortest.npy is exist.\n",
      "sofa\n",
      "ModelNet10/sofatest.npy is exist.\n",
      "toilet\n",
      "ModelNet10/toilettest.npy is exist.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "off_voxilize = False\n",
    "\n",
    "for t in train_or_test:\n",
    "    for cl in  tqdm(class_name):\n",
    "        print(cl)\n",
    "        \n",
    "        #すでにボクセル化が済んていればスキップ\n",
    "        if os.path.exists(vox_dir + \"x_train.npy\") == True:\n",
    "#             print(\"{} is exist.\".format(vox_dir + cl + t + \".npy\"))\n",
    "            off_voxilize = True\n",
    "            continue\n",
    "        \n",
    "        #すでに存在してるnpyファイルはスキップ\n",
    "        if os.path.exists(vox_dir + cl + t + \".npy\") == True:\n",
    "            print(\"{} is exist.\".format(data_dir + cl + t + \".npy\"))\n",
    "            continue\n",
    "        \n",
    "        num = glob.glob(data_dir+ cl +\"/\" + t + \"/*\")\n",
    "        for i in tqdm(range(len(num))):\n",
    "#             print(data_dir + cl + \"/\" + t +\"/\"+ str(i) + \".stl\")\n",
    "            if i ==0  :\n",
    "                vox_data = load_vox_off(data_dir + cl + \"/\" + t +\"/\"+ str(i) + extension)\n",
    "                continue\n",
    "            vox_data_ =  load_vox_off(data_dir + cl + \"/\" + t +\"/\"+ str(i) + extension)\n",
    "            \n",
    "            vox_data = np.append(vox_data, vox_data_, axis=0)\n",
    "        np.save(vox_dir + cl + t + \".npy\", vox_data)\n",
    "#クラス毎，TestTrain毎ボクセル化されたnpzが生成される（N,32,32,32,1）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#ココらへん可読性わるすぎ\n",
    "if not off_voxilize:\n",
    "    # データ整形\n",
    "    for t in train_or_test:\n",
    "        swich_npy = True \n",
    "        num_of_data = []#init\n",
    "        for cl in class_name:\n",
    "            if swich_npy == True:\n",
    "                npy = np.load(vox_dir  + cl + t + \".npy\")\n",
    "                num_of_data.append(npy.shape[0])\n",
    "                swich_npy = False\n",
    "            else:\n",
    "                npy_ = np.load(vox_dir  + cl + t + \".npy\")\n",
    "                npy = np.append(npy,npy_,axis=0)\n",
    "                num_of_data.append(npy_.shape[0])\n",
    "        if t == \"train\":\n",
    "            x_train = npy\n",
    "            y_train = num_of_data\n",
    "        if t == \"test\":\n",
    "            x_test = npy\n",
    "            y_test = num_of_data\n",
    "            \n",
    "        \n",
    "    # ラベルの生成\n",
    "    for t in train_or_test:\n",
    "        if t == \"train\":\n",
    "            for i in range(num_classes):\n",
    "                if i == 0:\n",
    "                    label = np.full(y_train[i] , i )    \n",
    "                else:\n",
    "                    label_ = np.full(y_train[i] , i )   \n",
    "                    label = np.append(label, label_, axis=0)\n",
    "            y_train = label\n",
    "\n",
    "        if t == \"test\":\n",
    "            for i in range(num_classes):\n",
    "                if i == 0:\n",
    "                    label = np.full(y_test[i] , i )    \n",
    "                else:\n",
    "                    label_ = np.full(y_test[i] , i )   \n",
    "                    label = np.append(label, label_, axis=0)\n",
    "            y_test = label\n",
    "\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "    y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "    y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "    \n",
    "    # 保存\n",
    "    npy = [\"x_train.npy\" , \"x_test.npy\" , \"y_train.npy\" , \"y_test.npy\"]\n",
    "    data = [x_train , x_test , y_train , y_test]\n",
    "    for i in range(len(npy)):\n",
    "        np.save(vox_dir + npy[i],data[i])\n",
    "        \n",
    "    # 不要ファイルの除去\n",
    "    for i in os.listdir(vox_dir):\n",
    "        if not i in npy:\n",
    "            os.remove(vox_dir + i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 距離の計算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94520e1beb964176a9ef098418503330",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=10), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bathtub\n",
      "chair\n",
      "dresser\n",
      "night_stand\n",
      "table\n",
      "bed\n",
      "desk\n",
      "monitor\n",
      "sofa\n",
      "toilet\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c986e82edafd44f6aebe13bb79533663",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=10), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bathtub\n",
      "chair\n",
      "dresser\n",
      "night_stand\n",
      "table\n",
      "bed\n",
      "desk\n",
      "monitor\n",
      "sofa\n",
      "toilet\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dist_calcrated = False\n",
    "for t in train_or_test:\n",
    "    for cl in  tqdm(class_name):\n",
    "        print(cl)\n",
    "        \n",
    "        #すでに距離の計算が済んていればスキップ\n",
    "        if os.path.exists(dist_dir + \"x_train.npy\") == True:\n",
    "#             print(\"{} is exist.\".format(vox_dir + cl + t + \".npy\"))\n",
    "            dist_calcrated = True\n",
    "            continue\n",
    "        \n",
    "        #すでに存在してるnpyファイルはスキップ\n",
    "        if os.path.exists(dist_dir + cl + t + \".npy\") == True:\n",
    "#             print(\"{} is exist.\".format(data_dir + cl + t + \".npy\"))\n",
    "            continue\n",
    "        \n",
    "        num = glob.glob(data_dir+ cl +\"/\" + t + \"/*\")\n",
    "        for i in tqdm(range(len(num))):\n",
    "            if i ==0  :\n",
    "                dist = calc_dist(data_dir + cl + \"/\" + t +\"/\"+ str(i) + extension)\n",
    "                dist = [dist]\n",
    "                continue\n",
    "            dist_ =  calc_dist(data_dir + cl + \"/\" + t +\"/\"+ str(i) + extension                                                                                                              )\n",
    "            \n",
    "            dist = np.append(dist, [dist_], axis=0)\n",
    "#             print(dist.shape)\n",
    "        np.save(dist_dir + cl + t + \".npy\", dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not dist_calcrated:\n",
    "    # データ整形\n",
    "    for t in train_or_test:\n",
    "        swich_npy = True \n",
    "        num_of_data = []#init\n",
    "        for cl in class_name:\n",
    "            if swich_npy == True:\n",
    "                npy = np.load(dist_dir  + cl + t + \".npy\")\n",
    "                num_of_data.append(npy.shape[0])\n",
    "                swich_npy = False\n",
    "            else:\n",
    "                npy_ = np.load(dist_dir  + cl + t + \".npy\")\n",
    "                npy = np.append(npy,npy_,axis=0)\n",
    "                num_of_data.append(npy_.shape[0])\n",
    "        if t == \"train\":\n",
    "            x_train = npy\n",
    "            y_train = num_of_data\n",
    "        if t == \"test\":\n",
    "            x_test = npy\n",
    "            y_test = num_of_data\n",
    "            \n",
    "        \n",
    "    # ラベルの生成\n",
    "    for t in train_or_test:\n",
    "        if t == \"train\":\n",
    "            for i in range(num_classes):\n",
    "                if i == 0:\n",
    "                    label = np.full(y_train[i] , i )    \n",
    "                else:\n",
    "                    label_ = np.full(y_train[i] , i )   \n",
    "                    label = np.append(label, label_, axis=0)\n",
    "            y_train = label\n",
    "\n",
    "        if t == \"test\":\n",
    "            for i in range(num_classes):\n",
    "                if i == 0:\n",
    "                    label = np.full(y_test[i] , i )    \n",
    "                else:\n",
    "                    label_ = np.full(y_test[i] , i )   \n",
    "                    label = np.append(label, label_, axis=0)\n",
    "            y_test = label\n",
    "\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "    y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "    y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "    \n",
    "    # 保存\n",
    "    npy = [\"x_train.npy\" , \"x_test.npy\" , \"y_train.npy\" , \"y_test.npy\"]\n",
    "    data = [x_train , x_test , y_train , y_test]\n",
    "    for i in range(len(npy)):\n",
    "        np.save(dist_dir + npy[i],data[i])\n",
    "        \n",
    "    # 不要ファイルの除去\n",
    "#     for i in os.listdir(dist_dir):\n",
    "#         if not i in npy:\n",
    "#             os.remove(dist_dir + i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
